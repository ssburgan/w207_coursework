{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNALzZOKTzOS"
   },
   "source": [
    "# Project 2: Digit Classification Naive Bayes\n",
    "\n",
    "**Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBRkdIUUTzOV"
   },
   "source": [
    "# Intro\n",
    "---\n",
    "In this project, you'll continue working on your own image recognition system for classifying digits. Read through the code and the instructions carefully and add your own code where indicated. Each problem can be addressed succinctly with the included packages -- please don't add any more. \n",
    "As always, you're welcome to work on the project in groups and discuss ideas over Slack, but <b> please prepare your own write-up (with your own code). </b>\n",
    "\n",
    "\n",
    "## Grading\n",
    "---\n",
    " - Make sure to answer every part in every question.\n",
    " - There are 6 questions, each equally weighted and an additional bonus question.\n",
    " - The extra credit question will be graded based on your attempts at feature engineering and resulting improved accuracy.\n",
    " - Read carefully what is asked including the notes.\n",
    " - Additional points may be deducted if:\n",
    "   - the code is not clean, well commented,\n",
    "  -  if the functions or answers are too long.\n",
    "\n",
    "## Requirements:\n",
    "---\n",
    "1. Comment your code.\n",
    "1. All graphs should have titles, label for each axis, and if needed a legend. It should be understandable on its own.\n",
    "1. All code must run on colab.research.google.com\n",
    "1. You should not import any additional libraries.\n",
    "1. Try and minimize the use of the global namespace (meaning keep things in functions).\n",
    "\n",
    "## Additional Reading and notes\n",
    "---\n",
    "If you're interested, check out these links related to digit recognition:\n",
    "\n",
    "* Yann Lecun's MNIST benchmarks: http://yann.lecun.com/exdb/mnist/\n",
    "* Stanford Streetview research and data: http://ufldl.stanford.edu/housenumbers/\n",
    "\n",
    "Finally, if you'd like to get started with Tensorflow, you can read through this tutorial: https://www.tensorflow.org/tutorials/keras/basic_classification. It uses a dataset called \"fashion_mnist\", which is identical in structure to the original digit mnist, but uses images of clothing rather than images of digits. The number of training examples and number of labels is the same. In fact, you can simply replace the code that loads \"fashion_mnist\" with \"mnist\" and everything should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r8u7GmsDTzOX"
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWOWoq_dTzOY"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1-CAjA9TzOa"
   },
   "source": [
    "Load the data. Notice that the data gets partitioned into training, development, and test sets. Also, a small subset of the training data called mini_train_data and mini_train_labels gets defined, which you should use in all the experiments below, unless otherwise noted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f9kPmCW0TzOb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (70000, 784)\n",
      "label shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the digit data from https://www.openml.org/d/554 or from default local location '~/scikit_learn_data/...'\n",
    "X, Y = fetch_openml(name='mnist_784', return_X_y=True, cache=False, as_frame=False)\n",
    "\n",
    "# Rescale grayscale values to [0,1].\n",
    "X = X / 255.0\n",
    "\n",
    "# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this\n",
    "# permutation to X and Y.\n",
    "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "print('data shape: ', X.shape)\n",
    "print('label shape:', Y.shape)\n",
    "\n",
    "# Set some variables to hold test, dev, and training data.\n",
    "test_data, test_labels = X[61000:], Y[61000:]\n",
    "dev_data, dev_labels = X[60000:61000], Y[60000:61000]\n",
    "train_data, train_labels = X[:60000], Y[:60000]\n",
    "mini_train_data, mini_train_labels = X[:1000], Y[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gr40-NstTzOl"
   },
   "source": [
    "### Question 1: Applying a smoothing using numpy.\n",
    "---\n",
    "\n",
    "A common image processing technique is to smooth an image by **blurring**. The idea is that the value of a particular pixel is estimated as the weighted combination of the original value and the values around it. Typically, the blurring is Gaussian, i.e., the weight of a pixel's influence is determined by a Gaussian function over the distance to the relevant pixel.\n",
    "\n",
    "1. Implement a simplified Gaussian blur filter by just using the 8 neighboring pixels like this: the smoothed value of a pixel is a weighted combination of the original value and the 8 neighboring values.\n",
    "1. Choose some weights as a starting point. Produce and evaluate four 1-Nearest Neighbor models by applying your blur filter in these ways:\n",
    "  1. Do not use the filter\n",
    "  1. Filter the training data but not the dev data\n",
    "  1. Filter the dev data but not the training data\n",
    "  1. Filter both training data and dev data\n",
    "1. Show the accuracies of the four models evaluated as described. What do you see? Can you explain why this is?\n",
    "1. Experiment with weights that makes one model's accuracy at least 0.9.\n",
    "\n",
    "Notes:\n",
    "* Train on the (filtered) mini train set.\n",
    "* Evaluate performance on the (filtered) dev set.\n",
    "* A good trick to simplify your code is to use numpy's pad function to add 0s around your original array so you don't have to deal with \"edge cases\".\n",
    "* In addition, you can use numpy to multiply and sum slices of two arrays.\n",
    "* [This notebook](https://colab.research.google.com/drive/1eJXTQLtREXQjQIsLOA9uCrBl6B049-pO) might help shows some example numpy code.\n",
    "* In general, [numpy operations will be much faster than for loops](https://colab.research.google.com/drive/1KJI4JtuIqVdyZHTTe_mAlKYA5XdLFp3_). \n",
    "* There are other Guassian blur filters available, for example in `scipy.ndimage.filters`. You are welcome to experiment with those, but in this question, please implement your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Q1(num_examples=10):\n",
    "\n",
    "  ### STUDENT START ###   \n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    x=0\n",
    "    for i in range(num_examples):\n",
    "        for j in range(num_examples):\n",
    "            x=x+1\n",
    "            #get all data matching current digit\n",
    "            image = mini_train_data[np.where(mini_train_labels==str(i))][j]\n",
    "            pixels = image.reshape((28, 28))\n",
    "            x#print(pixels)\n",
    "            #plot each digit\n",
    "            ax = fig.add_subplot(num_examples, num_examples, x, xticks=[], yticks=[],xlabel=i)\n",
    "            #setting the colormap, for example to black and white.\n",
    "            ax.imshow(pixels, cmap=plt.cm.binary) # cmap='gray'                                \n",
    "    plt.show()\n",
    "    \n",
    "  ### STUDENT END ###\n",
    "\n",
    "Q1(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebHGUU0eTzOl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aN3HZPGuTzOn"
   },
   "source": [
    "### Question 2: Modeling your data and experimenting with different Naive Bayes models. \n",
    "---\n",
    "\n",
    "1. Produce two Naive Bayes models and evaluate their performances.  Recall that Naive Bayes estimates P(feature|label), where each label is a categorical, not a real number.\n",
    "  1. For the first model, map pixel values to either 0 or 1, representing white or black - you should pre-process the data or use `BernoulliNB`'s `binarize` parameter to set the white/black separation threshold to 0.1.  Use `BernoulliNB` to produce the model.\n",
    "  1. For the second model, map pixel values to either 0, 1, or 2, representing white, gray, or black - you should pre-process the data, seting the white/gray/black separation thresholds to 0.1 and 0.9.  Here you'll likely need to implement a `trianarize` helper function. Since we are going beyond the binary representation of our features, you should use `MultinomialNB` to produce the model.\n",
    "1. Show the Bernoulli model accuracy and the Multinomial model accuracy.\n",
    "1. Does the multinomial version improve the results? Why or why not?\n",
    "\n",
    "Notes:\n",
    "* Train on the mini train set.\n",
    "* Evaluate performance on the dev set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution with binarization by explicit binarize function\n",
    "def binarize(data, threshold = 0.1):\n",
    "    binarized_data = np.zeros(data.shape)\n",
    "    binarized_data[data > threshold] = 1\n",
    "    binarized_data[data < threshold] = 0\n",
    "    return binarized_data\n",
    "\n",
    "def trianarize(data, threshold=[0.1, 0.9]):\n",
    "    trianarize_data = np.zeros(data.shape) \n",
    "    trianarize_data[data <= threshold[0]] = 0\n",
    "    trianarize_data[(data > threshold[0]) & (data < threshold[1])] = 1\n",
    "    trianarize_data[data >= threshold[1]] = 2\n",
    "    return trianarize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_1edM3ehTzOn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Classification report for BernoulliNB =====\n",
      "BernoulliNB accuracy=0.814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91       106\n",
      "           1       0.88      0.95      0.91       118\n",
      "           2       0.81      0.72      0.76       106\n",
      "           3       0.74      0.76      0.75        97\n",
      "           4       0.76      0.85      0.80        92\n",
      "           5       0.83      0.68      0.75        88\n",
      "           6       0.87      0.88      0.88       102\n",
      "           7       0.88      0.86      0.87       102\n",
      "           8       0.65      0.72      0.68        94\n",
      "           9       0.78      0.77      0.77        95\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.81      0.81      0.81      1000\n",
      "weighted avg       0.82      0.81      0.81      1000\n",
      "\n",
      "===== Classification report for MultinomialNB =====\n",
      "MultinomialNB accuracy=0.807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       106\n",
      "           1       0.87      0.87      0.87       118\n",
      "           2       0.79      0.73      0.75       106\n",
      "           3       0.76      0.76      0.76        97\n",
      "           4       0.78      0.79      0.78        92\n",
      "           5       0.87      0.68      0.76        88\n",
      "           6       0.90      0.89      0.90       102\n",
      "           7       0.96      0.86      0.91       102\n",
      "           8       0.56      0.72      0.63        94\n",
      "           9       0.72      0.79      0.75        95\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.81      0.80      0.81      1000\n",
      "weighted avg       0.82      0.81      0.81      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Q2():\n",
    "\n",
    "### STUDENT START ###\n",
    "    alpha = 1\n",
    "    binarized_train_data = binarize(mini_train_data) \n",
    "    binarized_dev_data = binarize(dev_data) \n",
    "\n",
    "    bern = BernoulliNB(alpha=alpha)\n",
    "    bern.fit(binarized_train_data, mini_train_labels)\n",
    "    \n",
    "    #Evaluate performance on the dev set.\n",
    "    pred = bern.predict(binarized_dev_data)\n",
    "\n",
    "    #Evaluate performance on the dev set.\n",
    "    acc = bern.score(binarized_dev_data, dev_labels)\n",
    "    print('===== Classification report for BernoulliNB =====')\n",
    "    print(f\"BernoulliNB accuracy={acc}\")\n",
    "    print(classification_report(dev_labels,pred))\n",
    "            \n",
    "    #For the second model trianarize\n",
    "    trianarize_train_data = trianarize(mini_train_data) \n",
    "\n",
    "    trianarize_dev_data = trianarize(dev_data) \n",
    "    mnb = MultinomialNB(alpha=alpha)\n",
    "    mnb.fit(trianarize_train_data, mini_train_labels)\n",
    "    \n",
    "    #Evaluate performance on the dev set.\n",
    "    predmnb = mnb.predict(trianarize_dev_data)\n",
    "\n",
    "    #Evaluate performance on the dev set.\n",
    "    acc = mnb.score(trianarize_dev_data, dev_labels)\n",
    "    print('===== Classification report for MultinomialNB =====')\n",
    "    print(f\"MultinomialNB accuracy={acc}\")\n",
    "    print(classification_report(dev_labels,predmnb))\n",
    "### STUDENT END ###\n",
    "Q2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bbbpzl5ETzOp"
   },
   "source": [
    "ANSWER:  Does the multinomial version improve the results? Why or why not? TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymzuQZwsTzOp"
   },
   "source": [
    "### Question 3: Applying the grid search technique.\n",
    "1. Search across several values of the LaPlace smoothing parameter (alpha) to find its effect on a Bernoulli Naive Bayes model's performance.  Show the accuracy at each alpha value.\n",
    "1. What is the best value for alpha? What is the accuracy when alpha is near 0? Is this what you'd expect?\n",
    "\n",
    "Notes:\n",
    "* Set binarization threshold to 0.\n",
    "* Train on the mini train set.\n",
    "* Evaluate performance by 5-fold cross-validation. \n",
    "* Use `GridSearchCV(..., ..., cv=..., scoring='accuracy', iid=False)` to vary alpha and evaluate performance by cross-validation.\n",
    "* Cross-validation is based on partitions of the training data, so results will be a bit different than if you had used the dev set to evaluate performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1qc1fG0OTzOq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5] END .......................alpha=1e-10;, score=0.875 total time=   0.0s\n",
      "[CV 2/5] END .......................alpha=1e-10;, score=0.755 total time=   0.0s\n",
      "[CV 3/5] END .......................alpha=1e-10;, score=0.840 total time=   0.0s\n",
      "[CV 4/5] END .......................alpha=1e-10;, score=0.760 total time=   0.0s\n",
      "[CV 5/5] END .......................alpha=1e-10;, score=0.840 total time=   0.0s\n",
      "[CV 1/5] END ......................alpha=0.0001;, score=0.860 total time=   0.0s\n",
      "[CV 2/5] END ......................alpha=0.0001;, score=0.765 total time=   0.0s\n",
      "[CV 3/5] END ......................alpha=0.0001;, score=0.845 total time=   0.0s\n",
      "[CV 4/5] END ......................alpha=0.0001;, score=0.785 total time=   0.0s\n",
      "[CV 5/5] END ......................alpha=0.0001;, score=0.840 total time=   0.0s\n",
      "[CV 1/5] END .......................alpha=0.001;, score=0.865 total time=   0.0s\n",
      "[CV 2/5] END .......................alpha=0.001;, score=0.775 total time=   0.0s\n",
      "[CV 3/5] END .......................alpha=0.001;, score=0.845 total time=   0.0s\n",
      "[CV 4/5] END .......................alpha=0.001;, score=0.790 total time=   0.0s\n",
      "[CV 5/5] END .......................alpha=0.001;, score=0.850 total time=   0.0s\n",
      "[CV 1/5] END ........................alpha=0.01;, score=0.855 total time=   0.0s\n",
      "[CV 2/5] END ........................alpha=0.01;, score=0.765 total time=   0.0s\n",
      "[CV 3/5] END ........................alpha=0.01;, score=0.845 total time=   0.0s\n",
      "[CV 4/5] END ........................alpha=0.01;, score=0.790 total time=   0.0s\n",
      "[CV 5/5] END ........................alpha=0.01;, score=0.855 total time=   0.0s\n",
      "[CV 1/5] END .........................alpha=0.1;, score=0.860 total time=   0.0s\n",
      "[CV 2/5] END .........................alpha=0.1;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END .........................alpha=0.1;, score=0.840 total time=   0.0s\n",
      "[CV 4/5] END .........................alpha=0.1;, score=0.780 total time=   0.0s\n",
      "[CV 5/5] END .........................alpha=0.1;, score=0.850 total time=   0.0s\n",
      "[CV 1/5] END .........................alpha=0.5;, score=0.855 total time=   0.0s\n",
      "[CV 2/5] END .........................alpha=0.5;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END .........................alpha=0.5;, score=0.835 total time=   0.0s\n",
      "[CV 4/5] END .........................alpha=0.5;, score=0.775 total time=   0.0s\n",
      "[CV 5/5] END .........................alpha=0.5;, score=0.840 total time=   0.0s\n",
      "[CV 1/5] END .........................alpha=1.0;, score=0.850 total time=   0.0s\n",
      "[CV 2/5] END .........................alpha=1.0;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END .........................alpha=1.0;, score=0.830 total time=   0.0s\n",
      "[CV 4/5] END .........................alpha=1.0;, score=0.770 total time=   0.0s\n",
      "[CV 5/5] END .........................alpha=1.0;, score=0.835 total time=   0.0s\n",
      "[CV 1/5] END .........................alpha=2.0;, score=0.845 total time=   0.0s\n",
      "[CV 2/5] END .........................alpha=2.0;, score=0.765 total time=   0.0s\n",
      "[CV 3/5] END .........................alpha=2.0;, score=0.825 total time=   0.0s\n",
      "[CV 4/5] END .........................alpha=2.0;, score=0.755 total time=   0.0s\n",
      "[CV 5/5] END .........................alpha=2.0;, score=0.835 total time=   0.0s\n",
      "[CV 1/5] END ........................alpha=10.0;, score=0.825 total time=   0.0s\n",
      "[CV 2/5] END ........................alpha=10.0;, score=0.765 total time=   0.0s\n",
      "[CV 3/5] END ........................alpha=10.0;, score=0.810 total time=   0.0s\n",
      "[CV 4/5] END ........................alpha=10.0;, score=0.725 total time=   0.0s\n",
      "[CV 5/5] END ........................alpha=10.0;, score=0.790 total time=   0.0s\n",
      "GridSearchCV accuracy=0.823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91       106\n",
      "           1       0.94      0.92      0.93       118\n",
      "           2       0.80      0.83      0.81       106\n",
      "           3       0.72      0.75      0.74        97\n",
      "           4       0.79      0.84      0.81        92\n",
      "           5       0.80      0.69      0.74        88\n",
      "           6       0.91      0.89      0.90       102\n",
      "           7       0.90      0.85      0.87       102\n",
      "           8       0.65      0.76      0.70        94\n",
      "           9       0.78      0.77      0.77        95\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.82      0.82      0.82      1000\n",
      "weighted avg       0.83      0.82      0.82      1000\n",
      "\n",
      "Best Params{'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "def Q3(alphas):\n",
    "\n",
    "### STUDENT START ###\n",
    "    #Set binarization threshold to 0.\n",
    "    binarized_train_data = binarize(mini_train_data, 0) \n",
    "    binarized_dev_data = binarize(dev_data, 0) \n",
    "\n",
    "\n",
    "    #params = []\n",
    "    #params = params.append(alphas.get(\"alpha\"))\n",
    "    \n",
    "    gs = GridSearchCV(estimator = BernoulliNB(),\n",
    "                      param_grid = alphas,\n",
    "                      scoring='accuracy',\n",
    "                      cv=5, verbose = 3)\n",
    "    \n",
    "    gs.fit(binarized_train_data, mini_train_labels)\n",
    "    \n",
    "    \n",
    "\n",
    "    #Evaluate performance on the dev set.\n",
    "    pred = gs.predict(binarized_dev_data)\n",
    "\n",
    "    #Evaluate performance on the dev set.\n",
    "    acc = gs.score(binarized_dev_data, dev_labels)\n",
    "    #print('===== Classification report for BernoulliNB =====')\n",
    "    print(f\"GridSearchCV accuracy={acc}\")\n",
    "    print(classification_report(dev_labels,pred))\n",
    "    print( \"Best Params\" + str(gs.best_params_))\n",
    "    # extract best estimator\n",
    "    #print(gs.best_estimator_)\n",
    "### STUDENT END ###\n",
    "\n",
    "alphas = {'alpha': [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "Q3(alphas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g4fnGFPTzOq"
   },
   "source": [
    "ANSWER: What is the best value for alpha? What is the accuracy when alpha is near 0? Is this what you'd expect?\n",
    "\n",
    "0 is not smoothing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ6MaDU6TzOr"
   },
   "source": [
    "### Question 4: Experimenting with Gaussian Naive Bayes\n",
    "---\n",
    "1. Produce a model using Guassian Naive Bayes, which is intended for real-valued features, and evaluate performance. You will notice that it does not work so well. \n",
    "1. Diagnose and explain the problem.\n",
    "1. Apply a simple fix so that the model accuracy is around the same as for a Bernoulli Naive Bayes model. \n",
    "1. Show the model accuracy before your fix and the model accuracy after your fix.  \n",
    "1. Explain your solution.\n",
    "\n",
    "Notes:\n",
    "* Train on the mini train set.\n",
    "* Evaluate performance on the dev set.\n",
    "* Take a look at var_smoothing argument for GaussianNB.\n",
    "* Feel free to  examine theta and sigma to better understand what's happening. In general though, **it is not a good idea to mess with internal variables of the model**. These are stored in the model's `theta_` and `sigma_` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XqW86YbKTzOr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Classification report for GaussianNB Before Smoothing =====\n",
      "GaussianNB accuracy=0.593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.88      0.81       106\n",
      "           1       0.87      0.81      0.84       118\n",
      "           2       0.59      0.44      0.51       106\n",
      "           3       0.64      0.29      0.40        97\n",
      "           4       0.78      0.32      0.45        92\n",
      "           5       0.30      0.67      0.41        88\n",
      "           6       0.74      0.85      0.79       102\n",
      "           7       0.92      0.57      0.70       102\n",
      "           8       0.38      0.11      0.17        94\n",
      "           9       0.43      0.91      0.58        95\n",
      "\n",
      "    accuracy                           0.59      1000\n",
      "   macro avg       0.64      0.58      0.57      1000\n",
      "weighted avg       0.65      0.59      0.58      1000\n",
      "\n",
      "===== Classification report for GaussianNB After Smoothing =====\n",
      "GaussianNB accuracy=0.764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       106\n",
      "           1       0.58      0.97      0.73       118\n",
      "           2       0.90      0.54      0.67       106\n",
      "           3       0.77      0.71      0.74        97\n",
      "           4       0.80      0.72      0.76        92\n",
      "           5       0.89      0.67      0.77        88\n",
      "           6       0.81      0.88      0.85       102\n",
      "           7       0.91      0.84      0.88       102\n",
      "           8       0.65      0.55      0.60        94\n",
      "           9       0.65      0.78      0.71        95\n",
      "\n",
      "    accuracy                           0.76      1000\n",
      "   macro avg       0.79      0.76      0.76      1000\n",
      "weighted avg       0.79      0.76      0.76      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#assumes features follow normal distribution\n",
    "def Q4():\n",
    "\n",
    "### STUDENT END ###\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(mini_train_data, mini_train_labels)\n",
    "    \n",
    "    #Evaluate performance on the dev set.\n",
    "    pred = gnb.predict(dev_data)\n",
    "\n",
    "    #Evaluate performance on the dev set.\n",
    "    acc = gnb.score(dev_data, dev_labels)\n",
    "    \n",
    "    print('===== Classification report for GaussianNB Before Smoothing =====')\n",
    "    print(f\"GaussianNB accuracy={acc}\")\n",
    "    print(classification_report(dev_labels,pred))\n",
    "    \n",
    "    gnb = GaussianNB(var_smoothing=.5)\n",
    "    gnb.fit(mini_train_data, mini_train_labels)    \n",
    "    #Evaluate performance on the dev set.\n",
    "    pred = gnb.predict(dev_data)\n",
    "\n",
    "    #Evaluate performance on the dev set.\n",
    "    acc = gnb.score(dev_data, dev_labels)\n",
    "    \n",
    "    print('===== Classification report for GaussianNB After Smoothing =====')\n",
    "    print(f\"GaussianNB accuracy={acc}\")\n",
    "    print(classification_report(dev_labels,pred))\n",
    "    \n",
    "### STUDENT END ###\n",
    "\n",
    "Q4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE5eQXsPTzOs"
   },
   "source": [
    "ANSWER: 1. Explain your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXx4SGNeTzOs"
   },
   "source": [
    "### Question 5: Naive Bayes as a generative model\n",
    "---\n",
    "Because Naive Bayes produces a generative model, you can use it to generate digit images.\n",
    "\n",
    "1. Produce a Bernoulli Naive Bayes model and then use it to generate a 10x10 grid with 10 example images of each digit. Each pixel output will be either 0 or 1: randomly generating a number and then comparing it to the estimated probability of the pixel being either 0 or 1.  Show the grid.\n",
    "1. How do the generated digit images compare to the training digit images?\n",
    "\n",
    "Notes:\n",
    "* You can use np.random.rand() to generate random numbers from a uniform distribution.\n",
    "* The estimated probability of each pixel being 0 or 1 is stored in the model's `feature_log_prob_` attribute. You can use `np.exp()` to convert a log probability back to a probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n",
      "==================================================\n",
      "[[2 1 3 4 0 3 3 4 0 2]\n",
      " [2 1 0 3 3 2 3 2 4 2]\n",
      " [1 3 1 2 3 1 1 1 1 3]\n",
      " [4 3 2 0 4 3 4 2 4 3]]\n",
      "==================================================\n",
      "[[1 3 1 2 3 1 1 1 1 3]]\n",
      "==================================================\n",
      "[3]\n",
      "[[-2.36712361 -2.77258872 -2.07944154 -1.85629799 -3.4657359  -2.07944154\n",
      "  -2.07944154 -1.85629799 -3.4657359  -2.36712361]\n",
      " [-2.36712361 -2.77258872 -3.4657359  -2.07944154 -2.07944154 -2.36712361\n",
      "  -2.07944154 -2.36712361 -1.85629799 -2.36712361]\n",
      " [-2.60268969 -1.9095425  -2.60268969 -2.19722458 -1.9095425  -2.60268969\n",
      "  -2.60268969 -2.60268969 -2.60268969 -1.9095425 ]\n",
      " [-2.05412373 -2.27726729 -2.56494936 -3.66356165 -2.05412373 -2.27726729\n",
      "  -2.05412373 -2.56494936 -2.05412373 -2.27726729]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randint(5, size=(4,10))\n",
    "y = np.array([1,2,3,4])\n",
    "clf = MultinomialNB()\n",
    "nb=clf.fit(x,y)\n",
    "print(x.shape)\n",
    "print(\"=\"*50)\n",
    "print(x)\n",
    "print(\"=\"*50)\n",
    "print(x[2:3])\n",
    "print(\"=\"*50)\n",
    "print(clf.predict(x[2:3]))\n",
    "print(nb.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "WDCNbv7ETzOt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8' '4' '1' '9' '6' '6' '3' '7' '9' '0' '4' '3' '1' '1' '3' '7' '1' '1'\n",
      " '4' '2' '2' '8' '9' '1' '7' '4' '9' '2' '1' '5' '3' '2' '8' '8' '1' '4'\n",
      " '8' '7' '4' '7' '4' '5' '6' '7' '7' '8' '4' '3' '1' '7' '2' '8' '1' '6'\n",
      " '2' '7' '1' '7' '9' '1' '7' '4' '0' '5' '0' '7' '1' '9' '8' '3' '7' '0'\n",
      " '6' '0' '6' '1' '8' '1' '9' '1' '0' '8' '6' '3' '1' '8' '2' '2' '1' '5'\n",
      " '6' '1' '8' '5' '4' '8' '4' '6' '8' '1' '1' '3' '9' '4' '9' '9' '2' '3'\n",
      " '0' '5' '2' '8' '8' '6' '9' '0' '5' '0' '3' '5' '2' '0' '4' '6' '7' '1'\n",
      " '0' '8' '0' '4' '1' '5' '0' '3' '7' '1' '0' '5' '6' '6' '2' '6' '7' '8'\n",
      " '3' '0' '3' '6' '1' '7' '8' '5' '6' '2' '7' '5' '2' '5' '1' '0' '1' '3'\n",
      " '2' '4' '9' '5' '1' '1' '1' '9' '5' '8' '1' '3' '8' '1' '2' '8' '5' '0'\n",
      " '6' '9' '8' '7' '8' '9' '8' '3' '2' '0' '5' '9' '8' '8' '3' '4' '8' '3'\n",
      " '7' '9' '2' '1' '2' '9' '4' '2' '4' '2' '6' '0' '9' '6' '7' '1' '9' '6'\n",
      " '7' '6' '7' '1' '0' '8' '0' '5' '3' '7' '0' '3' '8' '6' '0' '7' '9' '3'\n",
      " '7' '2' '8' '1' '9' '4' '5' '8' '4' '2' '7' '7' '2' '0' '7' '1' '3' '2'\n",
      " '4' '9' '0' '4' '0' '7' '3' '3' '9' '5' '3' '2' '8' '2' '9' '6' '5' '2'\n",
      " '0' '8' '5' '9' '3' '1' '1' '5' '0' '2' '8' '8' '8' '0' '2' '6' '5' '3'\n",
      " '1' '7' '8' '1' '6' '8' '4' '6' '7' '1' '7' '8' '5' '0' '7' '0' '8' '4'\n",
      " '3' '2' '4' '0' '8' '3' '8' '4' '0' '0' '0' '4' '4' '5' '4' '4' '2' '6'\n",
      " '8' '2' '0' '1' '3' '7' '3' '4' '8' '4' '1' '9' '9' '4' '7' '9' '4' '3'\n",
      " '1' '7' '2' '7' '5' '9' '7' '1' '4' '2' '8' '1' '4' '8' '6' '9' '3' '4'\n",
      " '1' '6' '8' '6' '8' '8' '6' '8' '5' '4' '1' '8' '5' '0' '4' '8' '4' '8'\n",
      " '2' '4' '3' '5' '1' '7' '1' '6' '8' '9' '9' '3' '6' '6' '2' '3' '6' '4'\n",
      " '5' '7' '8' '0' '1' '5' '6' '0' '9' '9' '3' '8' '4' '7' '0' '1' '3' '1'\n",
      " '5' '6' '8' '0' '0' '8' '6' '2' '5' '4' '0' '0' '6' '4' '9' '1' '4' '6'\n",
      " '4' '4' '2' '7' '2' '0' '2' '3' '1' '2' '7' '0' '9' '4' '1' '2' '4' '9'\n",
      " '3' '2' '9' '0' '9' '7' '8' '7' '4' '3' '8' '1' '6' '4' '3' '6' '2' '8'\n",
      " '6' '3' '6' '7' '0' '7' '3' '3' '6' '3' '6' '4' '3' '1' '0' '4' '0' '1'\n",
      " '8' '3' '7' '7' '0' '2' '7' '6' '2' '2' '1' '5' '0' '6' '3' '9' '8' '0'\n",
      " '9' '7' '6' '3' '9' '0' '8' '2' '8' '5' '7' '4' '2' '0' '2' '1' '1' '4'\n",
      " '6' '6' '9' '8' '4' '7' '1' '9' '9' '5' '5' '6' '4' '8' '9' '6' '7' '5'\n",
      " '2' '2' '8' '5' '1' '8' '1' '0' '2' '0' '9' '0' '5' '9' '9' '1' '8' '4'\n",
      " '2' '9' '5' '1' '8' '4' '6' '5' '6' '6' '3' '5' '7' '3' '0' '6' '2' '3'\n",
      " '0' '7' '9' '8' '6' '2' '4' '7' '1' '0' '5' '2' '7' '1' '6' '0' '2' '0'\n",
      " '8' '4' '6' '9' '0' '8' '2' '1' '1' '2' '6' '7' '3' '0' '3' '6' '4' '8'\n",
      " '5' '7' '0' '7' '1' '7' '7' '3' '9' '4' '3' '1' '4' '7' '6' '6' '9' '0'\n",
      " '6' '3' '2' '0' '2' '2' '0' '3' '5' '8' '0' '1' '0' '3' '4' '3' '6' '9'\n",
      " '4' '7' '2' '7' '8' '9' '1' '6' '8' '2' '9' '6' '3' '1' '4' '3' '0' '1'\n",
      " '5' '7' '7' '2' '5' '7' '1' '7' '2' '9' '0' '3' '2' '0' '5' '5' '6' '6'\n",
      " '1' '4' '1' '1' '5' '8' '6' '3' '8' '1' '4' '3' '3' '4' '1' '0' '6' '3'\n",
      " '7' '1' '1' '7' '0' '7' '3' '6' '3' '9' '0' '7' '9' '0' '0' '7' '9' '4'\n",
      " '9' '7' '1' '7' '0' '1' '2' '0' '5' '7' '9' '6' '0' '2' '6' '4' '7' '7'\n",
      " '0' '7' '6' '7' '3' '1' '2' '8' '9' '1' '6' '3' '1' '9' '4' '1' '5' '3'\n",
      " '6' '2' '7' '3' '1' '7' '1' '8' '8' '4' '1' '1' '4' '0' '9' '8' '6' '7'\n",
      " '9' '6' '6' '9' '8' '9' '0' '1' '7' '2' '1' '5' '6' '5' '3' '3' '2' '9'\n",
      " '3' '5' '3' '9' '3' '6' '3' '1' '3' '8' '6' '4' '2' '6' '1' '9' '8' '9'\n",
      " '0' '0' '3' '5' '1' '1' '6' '9' '2' '4' '7' '3' '4' '1' '5' '8' '1' '9'\n",
      " '5' '3' '4' '7' '1' '4' '8' '1' '8' '3' '1' '3' '9' '9' '6' '6' '2' '4'\n",
      " '3' '5' '4' '5' '0' '4' '4' '0' '9' '3' '1' '4' '4' '0' '4' '5' '9' '1'\n",
      " '8' '0' '2' '4' '5' '9' '6' '8' '6' '3' '9' '1' '2' '9' '6' '8' '0' '8'\n",
      " '4' '4' '4' '1' '7' '3' '0' '4' '5' '0' '8' '8' '0' '2' '5' '9' '5' '0'\n",
      " '9' '7' '4' '4' '7' '8' '6' '2' '8' '7' '4' '7' '3' '2' '9' '0' '0' '2'\n",
      " '9' '1' '6' '9' '4' '1' '3' '8' '1' '9' '1' '5' '1' '7' '2' '7' '0' '2'\n",
      " '2' '6' '2' '2' '7' '8' '6' '1' '5' '1' '8' '9' '6' '3' '1' '5' '3' '8'\n",
      " '7' '2' '8' '4' '3' '7' '6' '2' '5' '1' '7' '3' '4' '6' '1' '0' '9' '6'\n",
      " '6' '1' '1' '3' '9' '9' '6' '9' '0' '4' '3' '1' '3' '2' '0' '4' '1' '6'\n",
      " '8' '3' '4' '5' '6' '1' '2' '4' '2' '1']\n",
      "[[-4.53259949 -4.53259949 -4.53259949 ... -4.53259949 -4.53259949\n",
      "  -4.53259949]\n",
      " [-4.58496748 -4.58496748 -4.58496748 ... -4.58496748 -4.58496748\n",
      "  -4.58496748]\n",
      " [-4.53259949 -4.53259949 -4.53259949 ... -4.53259949 -4.53259949\n",
      "  -4.53259949]\n",
      " ...\n",
      " [-4.77068462 -4.77068462 -4.77068462 ... -4.77068462 -4.77068462\n",
      "  -4.77068462]\n",
      " [-4.82831374 -4.82831374 -4.82831374 ... -4.82831374 -4.82831374\n",
      "  -4.82831374]\n",
      " [-4.55387689 -4.55387689 -4.55387689 ... -4.55387689 -4.55387689\n",
      "  -4.55387689]]\n"
     ]
    }
   ],
   "source": [
    "def plot(examples):\n",
    "  ### STUDENT START ###   \n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    x=0\n",
    "    shape = examples.shape()\n",
    "    row = shape[0]\n",
    "    coumns =  shape[1]\n",
    "    for i in range(row):\n",
    "        for j in range(coumns):\n",
    "            x=x+1\n",
    "            #get all data matching current digit\n",
    "            image = mini_train_data[np.where(mini_train_labels==str(i))][j]\n",
    "            pixels = image.reshape((28, 28))\n",
    "            #plot each digit\n",
    "            ax = fig.add_subplot(num_examples, num_examples, x, xticks=[], yticks=[],xlabel=i)\n",
    "            #setting the colormap, for example to black and white.\n",
    "            ax.imshow(pixels, cmap=plt.cm.binary) # cmap='gray'                                \n",
    "    plt.show()\n",
    "    \n",
    "  ### STUDENT END ###\n",
    "\n",
    "\n",
    "\n",
    "def Q5(num_examples):\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "    binarized_train_data = binarize(mini_train_data) \n",
    "    binarized_dev_data = binarize(dev_data) \n",
    "\n",
    "\n",
    "    bern = BernoulliNB().fit(binarized_train_data, mini_train_labels)\n",
    "    pred = bern.predict(binarized_dev_data)\n",
    "    print(pred)\n",
    "    print(bern.feature_log_prob_)\n",
    "    #Evaluate performance on the dev set.\n",
    "   #pred = bern.predict(binarized_dev_data)\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "Q5(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ENXdzngTzOu"
   },
   "source": [
    "ANSWER: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRkUEi50TzOu"
   },
   "source": [
    "### Question 6: Model Calibraiton\n",
    "---\n",
    "Recall that:\n",
    " * a **strongly calibrated** classifier is rougly 90% accurate when it says it is 90% accurate. The model's estimated posterior probability of the predicted class is indeed 0.9. \n",
    " * A **weakly calibrated** classifier is more accurate when it \"says\" it is more accurate. The actual accuracy is higher when the model's estimated posterior probability is higher.\n",
    " * A **poorly calibrated** classifier has no positive correlation between the model's estimate posterior probability and the actual accuracy.\n",
    "\n",
    "1. Produce a Bernoulli Naive Bayes model.  \n",
    "1. Evaluate performance: \n",
    "  1. Partition the dev set into several buckets based on the estimated posterior probabilities of the predicted classes (predict_proba)\n",
    "    - Think of it as a bin in a histogram, where each bin groups a range of estimated posterior probabilities of the predicted classes (predict_proba).\n",
    "    - Then estimate the actual accuracy the classifier achieved for each bucket. \n",
    "    - So, for each prediction:\n",
    "       - Find the bucket whose range includes the estimated posterior probability, \n",
    "       - and update \"correct\" and \"total\" counters accordingly. \n",
    "       - Show the accuracy for each bucket.\n",
    "1. How would you characterize the calibration for this Bernoulli Naive Bayes model according to the definitions above?\n",
    "\n",
    "Notes:\n",
    "* Set LaPlace smoothing (alpha) to the optimal value (from part 8).\n",
    "* Set binarization threshold to 0.\n",
    "* Train on the mini train set.\n",
    "* Evaluate perfromance on the dev set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Gz3NDY9TzOu"
   },
   "outputs": [],
   "source": [
    "def Q6(buckets, correct, total):\n",
    "    \n",
    "### STUDENT START ###\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "buckets = [0.5, 0.9, 0.999, 0.99999, 0.9999999, 0.999999999, 0.99999999999, 0.9999999999999, 1.0]\n",
    "correct = [0 for i in buckets]\n",
    "total = [0 for i in buckets]\n",
    "\n",
    "Q6(buckets, correct, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PYLz-1STzOu"
   },
   "source": [
    "ANSWER: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIeZGen1TzOv"
   },
   "source": [
    "### Question 7 EXTRA CREDIT: Feature generation\n",
    "---\n",
    "1. Design new features to see if you can produce a Bernoulli Naive Bayes model with better performance.\n",
    "1. Show the accuracy of a model based on the original features and the accuracy of the model based on the new features.\n",
    "\n",
    "**Note that improving results is actually hard.**\n",
    "\n",
    "Here are a few ideas to get you started:\n",
    "- Try summing or averaging the pixel values in each row.\n",
    "- Try summing or averaging the pixel values in each column.\n",
    "- Try summing or averaging the pixel values in each square block. (pick various block sizes)\n",
    "- Try implementing [*maxpool*](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) features, taking a rolling maximum over sub-regions of a the image. \n",
    "- In any case, you can either transform the original data or add new \"features\" to it.\n",
    "\n",
    "Notes:\n",
    "* Train on the mini train set (enhanced to comprise the new features).\n",
    "* Evaulate performance on the dev set.\n",
    "* Ensure that your code is well commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GtYu5ezTzOv"
   },
   "outputs": [],
   "source": [
    "def Q7():\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "Q7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hOkGA6Rufd6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "firstname_lastname_project2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
